--- Chunk 0 ---
Introduction to Artiï¬cial Intelligence Marc Toussaint February 4, 2019 The majority of slides on search, CSP and logic are adapted from Stuart Russell . This is a direct concatenation and reformatting of all lecture slides and exercises from theArtiï¬cial Intelligence course (winter term 2018/19, U Stuttgart), including indexing to help prepare for exams. Double-starred** sections and slides are not relevant for the exam. Contents 1 Introduction 6 2 Search 13 Motivation & Outline 2.1 Problem Formulation & Examples . . . . . . . . . . . . . . . . . . . . 13 Example: Romania (2:3) Problem Deï¬nition: Deterministic, fully observable (2:5) 2.2 Basic Tree Search Algorithms . . . . . . . . . . . . . . . . . . . . . . . 15 Tree search implementation: states vs nodes (2:11) Tree Search: General Algorithm (2:12) Breadth-ï¬rst search (BFS) (2:15) Complexity of BFS (2:16) Uniform-cost search (2:17) Depth-ï¬rst search (DFS) (2:18) Complexity of DFS (2:19) Iterative deepening search (2:21) Complexity of Iterative Deepening Search (2:23) Graph search and re- peated states (2:25) 2.3 ASearch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 Best-ï¬rst Search (2:29) Asearch (2:31) A: Proof 1 of Optimality (2:33) Complexity of A(2:34) A: Proof 2 of Optimality (2:35) Admissible heuristics (2:37) Memory- bounded A(2:40) 1 2 Introduction to Artiï¬cial Intelligence, Marc Toussaint 3 Probabilities 31 Motivation & Outline Probabilities as (subjective) information calculus (3:2) Infer- ence: general meaning (3:5) Frequentist vs Bayesian (3:6) 3.1 Basic deï¬nitions . . . . . . . . . . . . . . . . . . .

--- Chunk 1 ---
. . . . . . . . . . . . 34 Deï¬nitions based on sets (3:8) Random variables (3:9) Probability distribution (3:10) Joint distribution (3:11) Marginal (3:11) Conditional distribution (3:11) Bayesâ€™ Theo- rem (3:13) Multiple RVs, conditional independence (3:14) 3.2 Probability distributions** . . . . . . . . . . . . . . . . . . . . . . . . . 36 Bernoulli and Binomial distributions (3:16) Beta (3:17) Multinomial (3:20) Dirichlet (3:21) Conjugate priors (3:25) 3.3 Distributions over continuous domain** . . . . . . . . . . . . . . . . . 41 Dirac distribution (3:28) Gaussian (3:29) Particle approximation of a distribution (3:33) Utilities and Decision Theory (3:36) Entropy (3:37) Kullback-Leibler diver- gence (3:38) 3.4 Monte Carlo methods** . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 Monte Carlo methods (3:40) Rejection sampling (3:41) Importance sampling (3:42) Studentâ€™s t, Exponential, Laplace, Chi-squared, Gamma distributions (3:44) 4 Bandits, MCTS, & Games 48 Motivation & Outline 4.1 Bandits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 Multi-armed Bandits (4:2) 4.2 Upper Conï¬dence Bounds (UCB) . . . . . . . . . . . . . . . . . . . . . 50 Exploration, Exploitation (4:7) Upper Conï¬dence Bound (UCB1) (4:8) 4.3 Monte Carlo Tree Search . . . . . . . . . . . . . . . . . . . . . . . . . . 52 Monte Carlo Tree

--- Chunk 2 ---
Search (MCTS) (4:14) Upper Conï¬dence Tree (UCT) (4:19) MCTS for POMDPs (4:20) 4.4 MCTS applied to POMDPs** . . . . . . . . . . . . . . . . . . . . . . . 55 4.5 Game Playing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 Minimax (4:29) Alpha-Beta Pruning (4:32) Evaluation functions (4:37) UCT for games (4:38) 4.6 Beyond bandits** . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 Global Optimization (4:43) GP-UCB (4:46) Active Learning (4:50) 4.7 Active Learning** . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 Introduction to Artiï¬cial Intelligence, Marc Toussaint 3 5 Dynamic Programming 69 Motivation & Outline 5.1 Markov Decision Process . . . . . . . . . . . . . . . . . . . . . . . . . . 69 Markov Decision Process (5:3) 5.2 Dynamic Programming . . . . . . . . . . . . . . . . . . . . . . . . . . 70 Value Function (5:6) Bellman optimality equation (5:10) Value Iteration (5:12) Q- Function (5:13) Q-Iteration (5:14) Proof of convergence of Q-Iteration (5:15) 5.3 Dynamic Programming in Belief Space . . . . . . . . . . . . . . . . . . 76 6 Reinforcement Learning 81 Motivation & Outline 6.1 Learning in

--- Chunk 3 ---
MDPs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84 Temporal difference (TD) (6:10) Q-learning (6:10) Proof of convergence of Q-learning (6:12) Eligibility traces (6:15) Model-based RL (6:28) 6.2 Exploration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94 Epsilon-greedy exploration in Q-learning (6:31) R-Max (6:33) Bayesian RL (6:35) Op- timistic heuristics (6:36) 6.3 Policy Search, Imitation, & Inverse RL** . . . . . . . . . . . . . . . . . 97 Policy gradients (6:41) Imitation Learning (6:43) Inverse RL (6:46) 7 Other models of interactive domains** 104 7.1 Basic Taxonomy of domain models . . . . . . . . . . . . . . . . . . . . 104 PDDL (7:6) Noisy Deictic Rules (7:9) POMDP (7:11) Dec-POMDP (7:20) Control (7:21) 8 Constraint Satisfaction Problems 113 Motivation & Outline 8.1 Problem Formulation & Examples . . . . . . . . . . . . . . . . . . . . 113 Inference (8:2) Constraint satisfaction problems (CSPs): Deï¬nition (8:3) Map- Coloring Problem (8:4) 8.2 Methods for solving CSPs . . . . . . . . . . . . . . . . . . . . . . . . . 116 Backtracking (8:10) Variable order: Minimum remaining values (8:15) Variable or- der: Degree heuristic (8:16) Value order: Least constraining value (8:17) Constraint propagation (8:18) Tree-structured CSPs (8:25) 4 Introduction to Artiï¬cial Intelligence, Marc Toussaint 9 Graphical Models 126 Motivation & Outline 9.1 Bayes Nets

--- Chunk 4 ---
and Conditional Independence . . . . . . . . . . . . . . . 126 Bayesian Network (9:5) Conditional independence in a Bayes Net (9:8) Inference: general meaning (9:13) 9.2 Inference Methods in Graphical Models . . . . . . . . . . . . . . . . . 133 Inference in graphical models: overview (9:22) Variable elimination (9:23) Factor graph (9:26) Belief propagation (9:32) Message passing (9:32) Loopy belief propa- gation (9:36) Junction tree algorithm** (9:38) Maximum a-posteriori (MAP) inference (9:42) Conditional random ï¬eld (9:43) Monte Carlo (9:45) Rejection sampling (9:46) Importance Sampling (9:48) Gibbs Sampling (9:50) 10 Dynamic Models 147 Motivation & Outline Markov Process (10:1) Hidden Markov Model (10:2) Filtering, Smoothing, Prediction (10:3) HMM: Inference (10:4) HMM inference (10:5) Kalman ï¬lter (10:8) 11 AI & Machine Learning & Neural Nets 154 Motivation & Outline Neural networks (11:8) 12 Explainable AI 165 13 Propositional Logic 173 Motivation & Outline 13.1 Syntax & Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173 Knowledge base: Deï¬nition (13:3) Wumpus World example (13:4) Logic: Deï¬nition, Syntax, Semantics (13:7) Propositional logic: Syntax (13:9) Propositional logic: Se- mantics (13:10) Logical equivalence (13:12) 13.2 Inference Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183 Inference (13:19) Horn Form (13:23) Modus Ponens (13:23) Forward chaining (13:24) Completeness of Forward Chaining (13:27) Backward Chaining (13:28) Conjunctive Normal Form (13:31) Resolution (13:31) Conversion to CNF (13:32) 14 First-Order Logic** 199 Motivation & Outline 14.1 The FOL language . . . . . . . . . .

--- Chunk 5 ---
. . . . . . . . . . . . . . . . . . . . 200 FOL: Syntax (14:4) Universal quantiï¬cation (14:6) Existential quantiï¬cation (14:6) 14.2 FOL Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203 Reduction to propositional inference (14:16) Uniï¬cation (14:19) Generalized Modus Ponens (14:20) Forward Chaining (14:21) Backward Chaining (14:27) Conversion to CNF (14:33) Resolution (14:35) Introduction to Artiï¬cial Intelligence, Marc Toussaint 5 15 Relational Probabilistic Modelling and Learning** 216 Motivation & Outline 15.1 STRIPS-like rules to model MDP transitions . . . . . . . . . . . . . . . 216 Markov Decision Process (MDP) (15:2) STRIPS rules (15:3) Planning Domain Deï¬ni- tion Language (PDDL) (15:3) Learning probabilistic rules (15:9) Planning with prob- abilistic rules (15:11) 15.2 Relational Graphical Models . . . . . . . . . . . . . . . . . . . . . . . . 220 Probabilistic Relational Models (PRMs) (15:20) Markov Logic Networks (MLNs) (15:24) The role of uncertainty in AI (15:31) 16 Exercises 228 16.1 Exercise 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228 16.2 Exercise 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230 16.3 Exercise 3 . . . . . . . . . . . . . . . .

--- Chunk 6 ---
. . . . . . . . . . . . . . . . . . . 232 16.4 Exercise 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234 16.5 Exercise 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236 16.6 Exercise 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239 16.7 Exercise 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241 16.8 Exercise 9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243 16.9 Exercise 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244 Index 246 on treesCSP graphical modelsMDPssequential decision problems search BFSpropositional logicFOL relational graphical models relational MDPs Reinforcement LearningHMMs MLmulti-agent MDPsMCTS utilitiesdeterministic learningprobabilisticpropositional relationalsequential decisions games banditsUCBconstraint propagation belief propagation msg. passing Active LearningDecision Theorydynamic programming V(s), Q(s,a)fwd/bwd chainingbacktracking fwd/bwd msg. passingFOLsequential assignment alpha/beta pruning minimax 6

--- Chunk 7 ---
Introduction to Artiï¬cial Intelligence, Marc Toussaint 1 Introduction (some slides based on Stuart Russellâ€™s AI course) What is intelligence? Maybe it is easier to ï¬rst ask what systems we actually talk about: â€“ Decision making â€“ Interacting with an environment Then deï¬ne objectives! â€“ Quantify what you consider good or successful â€“ Intelligence means to optimize... 1:1 Intelligence as Optimization? A cognitive scientist or psychologist: â€œWhy are you AI people always so ob- sessed with optimization? Humans are not optimal!â€ Thatâ€™s a total misunderstanding of what â€œbeing optimalâ€ means. Optimization principles are a means to describe systems: â€“ Feynmanâ€™s â€œunworldliness measureâ€ objective function â€“ Everything can be cast optimal â€“ under some objective â€“ Optimality principles are just a scientiï¬c means of formally describing systems and their behaviors (esp. in physics, economy, ... and AI) â€“Toussaint, Ritter & Brock: The Optimization Route to Robotics â€“ and Alternatives . KÂ¨unstliche Intelligenz, 2015 Generally, I would roughly distinguish three basic types of problems: â€“ Optimization â€“ Logical/categorial Inference (CSP , ï¬nd feasible solutions) â€“ Probabilistic Inference 1:2 What are interesting objectives? Learn to control all degrees of freedom of the environment that are controllable â€“ DOFs are mechanical/kinematics DOFs, objects, light/temperature, mood of hu- mans â€“ This objective is generic: no preferences, not limits â€“ Implies to actively go exploring and ï¬nding controllable DOFs Introduction to Artiï¬cial Intelligence, Marc Toussaint 7 â€“ Acting to Learning (instead of â€™Learning to Actâ€™ for a ï¬xed task) â€“ Related notions in other ï¬elds: (Bayesian) Experimental Design ,Active Learning , cu- riosity, intrinsic motivation At timeT, the system will be given a random task (e.g., random goal conï¬gu- ration of DOFs); the objective then is to reach it as quickly as possible 1:3 More on objectives The value alignment dilemma What are objectives that describe things

--- Chunk 8 ---
like â€œcreativityâ€, â€œempathyâ€, etc? Coming up with objective functions that imply desired behavior is a core part of AI research 1:4 Interactive domains We assume the agent is in interaction with a domain. â€“ The world is in a state st2S(see below on what that means) â€“ The agent senses observations yt2O â€“ The agent decides on an action at2A â€“ The world transitions to a new state st+1 The observationytdescribes all information received by the agent (sensors, also rewards, feedback, etc) if not explicitly stated otherwise (The technical term for this is a POMDP) 1:5 State The notion of state is often used imprecisely At any time t, we assume the world is in a state st2S stis astate description of a domain iff future observations yt+;t+>tare condi- tionally independent of all history observations yt ;t <tgivenstand future actionsat:t+: 8 Introduction to Artiï¬cial Intelligence, Marc Toussaint agents0 s1a0 s2a1 s3a2 a3 y0 y1 y2 y3 Notes: â€“ Intuitively, stdescribes everything about the world that is â€œrelevantâ€ â€“ Worlds do not have additional latent (hidden) variables to the state st 1:6 Examples What is a sufï¬cient deï¬nition of state of a computer that you interact with? What is a sufï¬cient deï¬nition of state for a thermostat scenario? (First, assume the â€™roomâ€™ is an isolated chamber.) What is a sufï¬cient deï¬nition of state in an autonomous car case? !in real worlds, the exact state is practically not representable !all models of domains will have to make approximating assumptions (e.g., about independencies) 1:7 How can agents be formally described? ...or, what formal classes of agents do exist? Basic alternative agent models: â€“ The agent maps yt7!at (stimulus-response mapping.. non-optimal) â€“ The agent stores all previous observations and maps f:y0:t;a0:t-17!at fis called agent function . This is the most general model, including the others as

--- Chunk 9 ---
special cases. â€“ The agent stores only the recent history and maps yt k:t;at k:t-17!at(crude, but may be a good heuristic) Introduction to Artiï¬cial Intelligence, Marc Toussaint 9 â€“ The agent is some machine with its own internal state nt, e.g., a computer, a ï¬nite state machine, a brain... The agent maps (nt-1;yt)7!nt(internal state update) and nt7!at â€“ The agent maintains a full probability distribution ( belief )bt(st)over the state, maps (bt-1;yt)7!bt(Bayesian belief update), and bt7!at 1:8 POMDP coupled to a state machine agent agents0 s1 s2 r1 r0 r2a2 y2 a1 y1 a0 y0n0 n1 n2 1:9 Multi-agent domain models (The technical term for this is a Decentralized POMDPs) (from Kumar et al., IJCAI 2011) This is a special type (simpliï¬cation) of a general DEC-POMDP Generally, this level of description is very general, but NEXP-hard Approximate methods can yield very good results, though 1:10 Summary â€“ AI is about: Systems that interact with the environment 10 Introduction to Artiï¬cial Intelligence, Marc Toussaint â€“ We distinguish between â€™systemâ€™ and â€™environmentâ€™ (cf. embodiment) â€“ We just introduced basic models of interaction â€“ A core part of AI research is to develop formal models for interaction Systems that aim to manipulate their invironment towards â€™desiredâ€™ states (op- timality) â€“ Optimality principles are a standard way to describe desired behaviors â€“ We sketched some interesting objectives â€“ Coming up with objective functions that imply desired behavior is a core part of AI research 1:11 Organisation 1:12 Vorlesungen der Abteilung MLR Bachelor: â€“ Grundlagen der K Â¨unstlichen Intelligenz (3+1 SWS) Master: â€“ Vertiefungslinie Intelligente Systeme (gemeinsam mit Andres Bruhn) â€“ WS: Maths for Intelligent Systems â€“ WS: Introduction to Robotics â€“ SS: Machine Learning â€“ (SS: Optimization) â€“ (Reinforcement Learning), (Advanced Robotics) â€“ Practical Course Robotics (SS) â€“ (Hauptseminare: Machine Learning (WS), Robotics (SS)) 1:13 Andres

